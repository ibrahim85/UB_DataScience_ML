{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill 17 WIKI Side Quest: Advanced architectures in neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by Toni Miranda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining CNN and RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN and RNN have been described by some of the classmates in this subquest. I would like to add on what has been said by briefly exploring when is a good idea to combine this 2 types of deep-learning models, CNN and RNN. This is on the basis of a book chapter: \"Deep Learning for Python\" by François Chollet, 2017 (Chapter 6. Deep Learning for text and sequences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Chollet (2017) two fundamental deep-learning algorithms for sequence processing are RNN and 1D CNN, the one-dimensional version of the 2D CNN explained already by classmates and typically used for computer vision. Applications of RNN and 1D CNN  algorithm combination include:\n",
    "- Document classification and timeseries classification (i.e. identifying the topic of an article or the author of a book).\n",
    "- Timeseries comparisons, such as estimating how closely related two documents are.\n",
    "- Sequence-to-sequence learning, such as decoding an English sentence into French\n",
    "- Sentiment analysis, such as classifying the sentiment of tweets or movie reviews as positive or negative (remember the Kaggle competition in this ML course?)\n",
    "- Timeseries forecasting, such as predicting the future weather at a certain location, given recent weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on 1D CNN for sequence data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layers introduced previously were 2D convolutions, extracting 2D patches from image tensors and applying an identical transformation to every patch. In the same way, 1D convolutions can be used, extracting local 1D patches (subsequences) from sequences as shown in the figure below (*Fig 1*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./How_1D_Conv_works.png)  \n",
    "*Fig 1.* How 1D convolution (1D CNN) works: each output timestep is obtained from a temporal patch in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CNNs and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence processing with 1-Dimension CNN (1D CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because 1D CNN process input patches independently, they aren’t sensitive to the order of the timesteps (beyond a local scale, the size of the convolution windows), unlike RNNs. To recognize longer-term patterns, it is possible to stack many convolution layers and pooling layers, but according to Chollet, that’s still a fairly weak way to induce order sensitivity because the CNN looks for patterns anywhere in the input timeseries and has no knowledge of the temporal position of a pattern it sees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One strategy to combine the speed and lightness of CNNs with the order-sensitivity of RNNs is to use a 1D convnet as a preprocessing step before an RNN. See figure below (*Fig 2*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./Combining_1D_CNN_and_RNN.png)  \n",
    "*Fig 2.* Combining a 1D CNN and an RNN for processing long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining CNNs and RNNs to process long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is especially beneficial when dealing with sequences that are so long they can’t realistically be processed with RNNs, such as sequences with thousands of steps. The CNN will turn the long input sequence into\n",
    "much shorter (downsampled) sequences of higher-level features. This sequence of extracted features then becomes the input to the RNN part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key take aways on sequence processing with CNN and combining RNN and CNN:\n",
    "- In the same way that 2D CNN perform well for processing visual patterns in 2D space, 1D CNN perform well for processing temporal patterns. They offer a faster alternative to RNNs on some problems, in particular natural-language processing tasks.\n",
    "- Typically, 1D CNN are structured much like their 2D equivalents from the world of computer vision.\n",
    "- Because RNNs are extremely expensive for processing very long sequences, but 1D CNN are cheap, it can be a good idea to use a 1D CNN as a preprocessing step before an RNN, shortening the sequence and extracting useful representations for the RNN to process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple of implementation\n",
    "*Note:* It requires Keras and Tensolflow installed. https://keras.io/#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set can be download from here: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
      "420551\n"
     ]
    }
   ],
   "source": [
    "#Inspecting the data of the Jena weather dataset\n",
    "import os\n",
    "data_dir = './'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "print(header)\n",
    "print(len(lines))\n",
    "\n",
    "#Parsing data\n",
    "import numpy as np\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "\n",
    "#Normalizing data\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generator yielding timeseries samples and their targets\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "              shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows),\n",
    "                           lookback // step,\n",
    "                           data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing the training, validation, and test generators\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step,\n",
    "                      batch_size=batch_size)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step,\n",
    "                    batch_size=batch_size)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step,\n",
    "                     batch_size=batch_size)\n",
    "\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "test_steps = (len(float_data) - 300001 - lookback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.4141"
     ]
    }
   ],
   "source": [
    "#Training and evaluating a simple 1D CNN on the Jena data\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,steps_per_epoch=500,epochs=20,validation_data=val_gen,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing higher-resolution data generators for the Jena dataset\n",
    "step = 3\n",
    "lookback = 720\n",
    "delay = 144\n",
    "\n",
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/toni/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, None, 32)          2272      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 32)          5152      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,697\n",
      "Trainable params: 13,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "500/500 [==============================] - 96s 193ms/step - loss: 0.3374 - val_loss: 0.2835\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 90s 181ms/step - loss: 0.3027 - val_loss: 0.2693\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 99s 198ms/step - loss: 0.2899 - val_loss: 0.2656\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 87s 173ms/step - loss: 0.2811 - val_loss: 0.2708\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 87s 175ms/step - loss: 0.2752 - val_loss: 0.2873\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 349s 698ms/step - loss: 0.2666 - val_loss: 0.2723\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 110s 220ms/step - loss: 0.2621 - val_loss: 0.2727\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 97s 193ms/step - loss: 0.2560 - val_loss: 0.2827\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 91s 182ms/step - loss: 0.2497 - val_loss: 0.2811\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 83s 166ms/step - loss: 0.2446 - val_loss: 0.2810\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 85s 170ms/step - loss: 0.2396 - val_loss: 0.2858\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 98s 196ms/step - loss: 0.2364 - val_loss: 0.2828\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 92s 183ms/step - loss: 0.2322 - val_loss: 0.2785\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 91s 181ms/step - loss: 0.2294 - val_loss: 0.2887\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 87s 174ms/step - loss: 0.2269 - val_loss: 0.2847\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 88s 177ms/step - loss: 0.2227 - val_loss: 0.2870\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 91s 182ms/step - loss: 0.2197 - val_loss: 0.2837\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 88s 175ms/step - loss: 0.2171 - val_loss: 0.2949\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 86s 172ms/step - loss: 0.2153 - val_loss: 0.2944\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 90s 181ms/step - loss: 0.2135 - val_loss: 0.2918\n"
     ]
    }
   ],
   "source": [
    "#Model combining a 1D convolutional base and a GRU layer\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                        input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()\n",
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                      steps_per_epoch=500,\n",
    "                                      epochs=20,\n",
    "                                      validation_data=val_gen,\n",
    "                                      validation_steps=val_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
